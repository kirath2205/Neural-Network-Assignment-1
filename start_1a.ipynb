{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "start_1a.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Neural-Network-Assignment-1/blob/main/start_1a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM8unKH7nSIM"
      },
      "source": [
        "import tensorflow\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import statistics\n",
        "\n",
        "# tensorflow libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Input\n",
        "\n",
        "# sklearn libraries are useful for preprocessing, performance measures, etc.\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmTBl3xoqA9g"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An4b9d12p8u2"
      },
      "source": [
        "df = pd.read_csv('./features_30_sec.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHZQbQVzqGIA"
      },
      "source": [
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zp_RDMgqL9Z"
      },
      "source": [
        "#Question 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fNTd9t7qHCI"
      },
      "source": [
        "columns_to_drop = ['label','filename', 'length']\n",
        "\n",
        "def prepare_dataset_question1(df, columns_to_drop, test_size, random_state):\n",
        "\n",
        "    # Encode the labels from 0 to n_classes-1  \n",
        "    label_encoder = preprocessing.LabelEncoder()\n",
        "    df['label'] = label_encoder.fit_transform(df['label'])\n",
        "  \n",
        "    # devide data to train and test\n",
        "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "    \n",
        "    # scale the training inputs\n",
        "    x_train = df_train.drop(columns_to_drop,axis=1)\n",
        "    y_train = df_train['label'].to_numpy()\n",
        "    \n",
        "    standard_scaler = preprocessing.StandardScaler()\n",
        "    x_train_scaled = standard_scaler.fit_transform(x_train)\n",
        "\n",
        "    #scale and prepare testing data\n",
        "    x_test = df_test.drop(columns_to_drop,axis=1)\n",
        "    x_test_scaled = standard_scaler.transform(x_test)\n",
        "    y_test = df_test['label'].to_numpy() \n",
        "  \n",
        "    return x_train_scaled, y_train, x_test_scaled, y_test"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwr6HuVTLJdQ"
      },
      "source": [
        "X_train, y_train, X_test, y_test = prepare_dataset_question1(df, columns_to_drop, test_size=0.3, random_state=0)\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHzGpNhMLRKK"
      },
      "source": [
        "def build_model(total_classes,input_shape,hidden_layer_neurons=16,hidden_layers=1,dropout_probability=0.3,optimizer='adam'):\n",
        "\n",
        "  model=tf.keras.models.Sequential()\n",
        "  model.add(keras.layers.Input(shape=input_shape))\n",
        "  for i in range(hidden_layers):\n",
        "    model.add(keras.layers.Dense(hidden_layer_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dropout(dropout_probability))\n",
        "  model.add(keras.layers.Dense(total_classes, activation='softmax'))\n",
        "  model.compile(optimizer=optimizer,metrics=['accuracy'],loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_model(model,epochs,X_train,y_train,X_test,y_test,batch_size=1,callbacks=[]):\n",
        "\n",
        "  history=model.fit(X_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_test,y_test),callbacks=callbacks)\n",
        "  return history\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKTjIoXON16_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840849c3-625f-4895-f577-030e1f92aa0c"
      },
      "source": [
        "history={}\n",
        "model=build_model(10,X_train[0].shape)\n",
        "history['question-1']=train_model(model,50,X_train,y_train,X_test,y_test)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question-1': <keras.callbacks.History object at 0x7f732054d150>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rp6THVHUO5g"
      },
      "source": [
        "plt.plot(history['question-1'].history['accuracy'])\n",
        "plt.plot(history['question-1'].history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history['question-1'].history['loss'])\n",
        "plt.plot(history['question-1'].history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IorHzJhS7Cq"
      },
      "source": [
        "#Question 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyjeIqAAWynN"
      },
      "source": [
        "def prepare_dataset_question2(df, columns_to_drop):\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "  X = df.drop(columns_to_drop,axis=1)\n",
        "  Y = df['label'].to_numpy()\n",
        "\n",
        "  standard_scaler = preprocessing.StandardScaler()\n",
        "  X_scaled = standard_scaler.fit_transform(X)\n",
        "\n",
        "  return X_scaled,Y\n",
        "\n",
        "class TimeHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPwdWcysUzuQ"
      },
      "source": [
        "X , Y = prepare_dataset_question2(df , columns_to_drop)\n",
        "n_splits=3\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "batch_sizes=[1,4,8,16,32, 64]\n",
        "total_classes=10\n",
        "input_shape=X[0].shape\n",
        "history={}\n",
        "k_fold=1\n",
        "epochs=50\n",
        "epoch_times={}\n",
        "\n",
        "for element in batch_sizes:\n",
        "\n",
        "  epoch_times[element]=[]\n",
        "\n",
        "for train,test in kf.split(X):\n",
        "\n",
        "  X_train , X_test = X[train] , X[test]\n",
        "  y_train , y_test = Y[train] , Y[test]\n",
        "\n",
        "  for batch_size in batch_sizes:\n",
        "\n",
        "    model=build_model(total_classes,input_shape)\n",
        "    time_callback = TimeHistory()\n",
        "    history[(batch_size,k_fold)] = train_model(model,epochs,X_train,y_train,X_test,y_test,batch_size,[time_callback])\n",
        "\n",
        "    for element in time_callback.times:\n",
        "      epoch_times[batch_size].append(element)\n",
        "\n",
        "  k_fold+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef-ocmK06jcB"
      },
      "source": [
        "mean_cross_validation_accuracies=[]\n",
        "for batch_size in batch_sizes:\n",
        "  temp=[]\n",
        "  for key in history.keys():\n",
        "    if(key[0]==batch_size):\n",
        "      temp.append(history[key].history['val_accuracy'])\n",
        "  mean_accuracy=[0]*epochs\n",
        "  for element in temp:\n",
        "    for epoch in range(epochs):\n",
        "      mean_accuracy[epoch]+=element[epoch]\n",
        "  for epoch in range(epochs):\n",
        "    mean_accuracy[epoch]=mean_accuracy[epoch]/n_splits\n",
        "  mean_cross_validation_accuracies.append(mean_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGdXRbkFV7G-"
      },
      "source": [
        "median_epoch_times={}\n",
        "for batch_size in epoch_times.keys():\n",
        "  median_epoch_times[batch_size]=float(\"{:.2f}\".format(statistics.median(epoch_times[batch_size])))\n",
        "median_epoch_time_table=pd.DataFrame(median_epoch_times.items(),columns=['Batch_size','Mean_time'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU247Eg6e2P8"
      },
      "source": [
        "median_epoch_time_table.index = median_epoch_time_table.index + 1\n",
        "median_epoch_time_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5t23xB6DbYZ"
      },
      "source": [
        "epoch_list=[(i+1) for i in range(epochs)]\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('mean cross-validation accuracies for different batch-sizes')\n",
        "for index in range(len(batch_sizes)):\n",
        "  batch_size=batch_sizes[index]\n",
        "  mean_accuracy_list=mean_cross_validation_accuracies[index]\n",
        "  plt.plot(epoch_list, mean_accuracy_list, label = 'batch size of '+str(batch_sizes[index]))\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRz9U9I-Y14V"
      },
      "source": [
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "optimal_batch_size=64\n",
        "train_accuracy=[0]*epochs\n",
        "test_accuracy=[0]*epochs\n",
        "temp_train_accuracy=[]\n",
        "temp_test_accuracy=[]\n",
        "for key in history.keys():\n",
        "  if(key[0]==optimal_batch_size):\n",
        "    temp_train_accuracy.append(history[key].history['accuracy'])\n",
        "    temp_test_accuracy.append(history[key].history['val_accuracy'])\n",
        "for i in range(n_splits):\n",
        "  for epoch in range(epochs):\n",
        "    train_accuracy[epoch]+=temp_train_accuracy[i][epoch]\n",
        "    test_accuracy[epoch]+=temp_test_accuracy[i][epoch]\n",
        "for epoch in range(epochs):\n",
        "  train_accuracy[epoch]=train_accuracy[epoch]/n_splits\n",
        "  test_accuracy[epoch]=test_accuracy[epoch]/n_splits\n",
        "\n",
        "plt.title('test and train accuracy for batch size - '+str(optimal_batch_size))\n",
        "plt.plot(epoch_list,train_accuracy,label='training accuracy')\n",
        "plt.plot(epoch_list,test_accuracy,label='test accuracy')\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHBXuIgAEcj"
      },
      "source": [
        "#Question 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rje5XuW8AHm5"
      },
      "source": [
        "hidden_layer_neuron_list=[8, 16, 32, 64]\n",
        "for train,test in kf.split(X):\n",
        "\n",
        "  X_train , X_test = X[train] , X[test]\n",
        "  y_train , y_test = Y[train] , Y[test]\n",
        "\n",
        "  for hidden_layer_neurons in hidden_layer_neuron_list:\n",
        "\n",
        "    model=build_model(total_classes,input_shape,hidden_layer_neurons)\n",
        "    history[(hidden_layer_neurons,k_fold)] = train_model(model,epochs,X_train,y_train,X_test,y_test,optimal_batch_size,[time_callback])\n",
        "\n",
        "  k_fold+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVN3p1lDd5R-"
      },
      "source": [
        "mean_cross_validation_accuracies=[]\n",
        "for hidden_layer_neurons in hidden_layer_neuron_list:\n",
        "  temp=[]\n",
        "  for key in history.keys():\n",
        "    if(key[0]==hidden_layer_neurons):\n",
        "      temp.append(history[key].history['val_accuracy'])\n",
        "  mean_accuracy=[0]*epochs\n",
        "  for element in temp:\n",
        "    for epoch in range(epochs):\n",
        "      mean_accuracy[epoch]+=element[epoch]\n",
        "  for epoch in range(epochs):\n",
        "    mean_accuracy[epoch]=mean_accuracy[epoch]/n_splits\n",
        "  mean_cross_validation_accuracies.append(mean_accuracy)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiMDEaAgegyH"
      },
      "source": [
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('mean cross-validation accuracies for different batch-sizes')\n",
        "for index in range(len(hidden_layer_neuron_list)):\n",
        "  hidden_layer_neurons=hidden_layer_neuron_list[index]\n",
        "  mean_accuracy_list=mean_cross_validation_accuracies[index]\n",
        "  plt.plot(epoch_list, mean_accuracy_list, label = 'Total hidden neurons of '+str(hidden_layer_neuron_list[index]))\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGpGsJWFe4qF"
      },
      "source": [
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "optimal_neurons=64\n",
        "train_accuracy=[0]*epochs\n",
        "test_accuracy=[0]*epochs\n",
        "temp_train_accuracy=[]\n",
        "temp_test_accuracy=[]\n",
        "history={}\n",
        "for key in history.keys():\n",
        "  if(key[0]==optimal_neurons):\n",
        "    temp_train_accuracy.append(history[key].history['accuracy'])\n",
        "    temp_test_accuracy.append(history[key].history['val_accuracy'])\n",
        "for i in range(n_splits):\n",
        "  for epoch in range(epochs):\n",
        "    train_accuracy[epoch]+=temp_train_accuracy[i][epoch]\n",
        "    test_accuracy[epoch]+=temp_test_accuracy[i][epoch]\n",
        "for epoch in range(epochs):\n",
        "  train_accuracy[epoch]=train_accuracy[epoch]/n_splits\n",
        "  test_accuracy[epoch]=test_accuracy[epoch]/n_splits\n",
        "\n",
        "plt.title('test and train accuracy for '+str(optimal_neurons)+' neurons')\n",
        "plt.plot(epoch_list,train_accuracy,label='training accuracy')\n",
        "plt.plot(epoch_list,test_accuracy,label='test accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haudiBbemFoP"
      },
      "source": [
        "#Question-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-on9TM-ofczc"
      },
      "source": [
        "X_train, y_train, X_test, y_test = prepare_dataset_question1(df, columns_to_drop, test_size=0.3, random_state=0)\n",
        "total_classes=10\n",
        "image_shape=X_train[0].shape\n",
        "hidden_layers=2\n",
        "history={}\n",
        "batch_size=1\n",
        "model=build_model(total_classes,image_shape,optimal_neurons,hidden_layers)\n",
        "history['question-4']=train_model(model,50,X_train,y_train,X_test,y_test,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2PhEDOgnUfk"
      },
      "source": [
        "plt.plot(history['question-4'].history['accuracy'])\n",
        "plt.plot(history['question-4'].history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqxx5ilYnt3U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}